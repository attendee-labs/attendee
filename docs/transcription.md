# Transcription

Attendee offers two methods for real-time meeting transcription: **Third-party-based Transcription** and **Closed Caption-based Transcription**. Both methods allow you to receive real-time updates via webhooks. Also, both methods have perfect speaker identification, also known as diarization.

For an example of a simple web application that uses Attendee to transcribe meeting audio in real time, see the [real time transcription example](https://github.com/attendee-labs/realtime-transcription-example) repository.

## Third-party-based Transcription

This method relies on access to a per-speaker audio stream for each participant. Attendee identifies when a participant starts and stops speaking. When a participant pauses for a few seconds, the audio segment is sent to a third-party transcription provider for processing.

### Latency
The latency of third-party-based transcription is dependent on two main factors: the time it takes for the third-party provider to transcribe the audio, and the size of the audio segment itself. If a participant speaks for a long time without pausing, the audio segment sent for transcription will be large, increasing processing time. These two factors mean that third-party-based transcription generally has higher latency compared to closed caption-based transcription.

### Quality

Third-party-based transcription is generally of higher quality than closed caption-based transcription.

### Supported providers

- Deepgram
- OpenAI
- Gladia
- Assembly AI
- Sarvam
- ElevenLabs
- Kyutai Labs
- Azure Speech-to-Text
- Custom Async (Bring Your Own Platform)

See the [API reference](https://docs.attendee.dev/api-reference#tag/bots/POST/api/v1/bots) for supported parameters for configuring the transcription providers.

### Cost

Third-party-based transcription incurs costs from the transcription provider. Attendee uses an API key to call the transcription provider that you provide in the Credentials section of the dashboard.

## Closed Caption-based Transcription

This method takes advantage of the built-in closed captioning feature of the meeting platfom. Attendee captures these captions as they are generated by the platform.

### Latency
This method offers lower latency. Captions are captured as soon as they are generated by the platform.

### Cost

Closed caption-based transcription is free.


## Choosing the Right Method

| Feature                  | Third-party-based Transcription                                     | Closed Caption-based Transcription                              |
| ------------------------ | ------------------------------------------------------------- | --------------------------------------------------------------- |
| **Source**               | Per-participant audio segments                  | Built-in captions from the meeting platform (Zoom, Google Meet) |
| **Transcription Quality**| High (depends on the provider, e.g., OpenAI, Deepgram)        | Generally lower than third-party-based transcription
| **Word-level timestamps**| Supported by all providers except OpenAI                         | No.
| **Speaker Diarization**  | Yes, perfect speaker identification.                          | Yes, perfect speaker identification.                            |
| **Latency**              | Higher latency due to provider processing and segment size.     | Lower latency, near-instantaneous.                              |
| **Cost**                 | Incurs costs from third-party transcription providers.        | No additional costs.                     |
| **Setup**                | Requires configuring a third-party transcription provider.    | No setup required.                  |

## Adding transcription providers in the dashboard

For third-party-based transcription, you need to add your API Key for a provider like Deepgram, OpenAI, Gladia, or Assembly AI in the Settings > Credentials page.

## Transcription errors

If you are using third-party-based transcription, you may encounter errors from the transcription provider. These errors are visible in the bot detail page in the dashboard, in the transcription section.

Additionally, the `post-processing complete` bot event will contain a list of transcription errors in the event metadata.

## Configuring transcription in the API call

You can configure transcription settings when creating a bot. This includes selecting the transcription provider and provider-specific options like language, model, etc. See the [API reference](https://docs.attendee.dev/api-reference#tag/bots/POST/api/v1/bots) for details. You will set the parameters in the `transcription_settings` object of the `create bot` request body. It will have the form

```json
{
    "chosen transcription provider": {
        "provider-specific parameters"
    }
}
```

For example, if you want to use Deepgram with english and the nova-2 model, you will set the `transcription_settings` to:

```json
{
    "deepgram": {
        "language": "en-US",
        "model": "nova-2"
    }
}
```

## Setting up webhooks for real time transcription

You can set up webhooks for real time transcription in the dashboard. Go to the Settings > Webhooks page and click the 'Create Webhook' button.

Make sure the `transcript.update` trigger is enabled for your webhook. This will fire a webhook event every time a new utterance is added to the transcript. See the [webhooks](webhooks.md#payload-for-transcriptupdate-trigger) page for more details on the webhook payload.

## Fetching transcripts during and after the meeting

You can fetch transcripts during and after the meeting, by calling the `/transcript` endpoint. See the [API reference](https://docs.attendee.dev/api-reference#tag/bots/GET/api/v1/bots/{object_id}/transcript) for details.

## Multilingual transcription

All transcription methods can transcribe audio in different languages, but some methods support different languages than others. See the [API reference](https://docs.attendee.dev/api-reference#tag/bots/POST/api/v1/bots) for details on how to specify the language.

All third-party transcription providers support automatic language detection, but closed caption-based transcription does not. Some third-party providers have the ability to transcribe audio where the speaker is switching languages in the middle of a sentence, see the list below for details.

## Choosing the right transcription provider

### Deepgram

Cheap price, good quality, and fast, the only downside is it doesn't support as many languages as some of the other providers.

Can transcribe audio where the speaker is switching languages in the middle of a sentence.

$200 in free credits for new users.

### Gladia

Similar to Deepgram, but more expensive and supports more languages.

Can transcribe audio where the speaker is switching languages in the middle of a sentence.

10 hours of free transcription each month.

### Assembly AI

Similar to Deepgram in price and quality but lacks the ability to transcribe audio where the speaker is switching languages in the middle of a sentence. Very accurate word-level timestamps.

$50 in free credits for new users.

### OpenAI

Cheaper then the other providers, but less accurate and often chooses the wrong language when the language is not specified in advance. Can transcribe audio where the speaker is switching languages in the middle of a sentence. Lacks word-level timestamps.

#### Note on custom OpenAI proxy servers

To use a custom OpenAI-compatible endpoint (such as a proxy server or alternative model provider), set these environment variables:

- `OPENAI_BASE_URL`: The base URL for your custom endpoint (default: `https://api.openai.com/v1`)
- `OPENAI_MODEL_NAME`: The model name to use for transcription (default: `gpt-4o-transcribe`)

Example: `OPENAI_BASE_URL=https://your-proxy.com/v1` and `OPENAI_MODEL_NAME=whisper-large-v3`

### Azure Speech-to-Text

Microsoft's enterprise-grade speech recognition with support for 140+ languages. Uses Azure Fast Transcription API for synchronous, high-quality transcription of audio segments.

**Features:**
- Fast synchronous transcription (typically 15 seconds for 10 minutes of audio)
- Automatic language detection from candidate languages
- Profanity filtering options
- Phrase lists for vocabulary hints
- Word-level timestamps
- Support for audio files up to 2 hours / 300 MB

$5 in free credits per month for new users, then pay-as-you-go pricing.

#### Configuration

Azure Speech-to-Text requires credentials to be set up separately via the Credentials API. Configuration options can be provided per-bot in the API request.

**Step 1: Add Azure Credentials** (One-time setup per project)

Via Web Dashboard:
1. Go to Project Settings â†’ Credentials
2. Click "Add Credentials" for Azure Speech-to-Text
3. Provide:
   - **Subscription Key** (required)
   - **API Version** (required, e.g., "2024-11-15")
   - **Region** (required, e.g., "eastus")

**Step 2: Create Bots with Azure**

Optional fields in `transcription_settings.azure`:
- `candidate_languages` (array): List of BCP-47 locale codes for automatic language detection (2-10 languages). Defaults to `["en-US", "es-ES", "fr-FR", "ar-SA", "ar-TN"]` if not specified.
- `phrase_list` (array): List of phrases for vocabulary hints
- `profanity_option` (string): `"Raw"`, `"Masked"` (default), or `"Removed"`
- `enable_disfluency_removal` (boolean): Remove filler words like "um", "uh"
- `custom_endpoint_id` (string): Endpoint ID for custom speech models

**Example API request (minimal - uses defaults):**
```bash
curl -X POST https://api.attendee.ai/v1/bots \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "meeting_url": "https://meet.google.com/abc-defg-hij",
    "bot_name": "My Bot",
    "transcription_settings": {
      "azure": {}
    }
  }'
```

**Example API request (with custom configuration):**
```bash
curl -X POST https://api.attendee.ai/v1/bots \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "meeting_url": "https://meet.google.com/abc-defg-hij",
    "bot_name": "My Bot",
    "transcription_settings": {
      "azure": {
        "candidate_languages": ["en-US", "fr-FR", "es-ES", "ar-AE"],
        "phrase_list": ["Microsoft", "Azure", "Attendee"],
        "profanity_option": "Masked",
        "enable_disfluency_removal": true
      }
    }
  }'
```

**Note:** Credentials (subscription_key, endpoint, api_version, region) must be set up via the Credentials API before creating bots with Azure transcription. They cannot be passed in the bot creation request.

**Supported locales:**
Common supported locales include:
- `en-US` (English - US)
- `fr-FR` (French - France)
- `es-ES`, `es-MX` (Spanish)
- `ar-AE`, `ar-BH`, `ar-EG` (Arabic)
- `de-DE` (German)
- `it-IT` (Italian)
- `ja-JP` (Japanese)
- `ko-KR` (Korean)
- `pt-BR` (Portuguese - Brazil)
- `hi-IN` (Hindi)

See [Azure's language support documentation](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/language-support) for the complete list of supported locales.

**Validation:**
All required environment variables are validated when Azure is selected as the transcription provider. If any required variable is missing, transcription will fail with a clear error message listing all missing variables.

**How it works:**
1. Audio segments are accumulated per participant (from speech start until 3 seconds of silence or max size limit)
2. Each segment is sent to Azure Fast Transcription API as a WAV file
3. Azure detects the language from your candidate languages list
4. Transcription results are returned synchronously with word-level timestamps
5. Results are saved and webhooks are triggered (if configured)

### Custom Async (Bring Your Own Platform)

For Attendee self-hosters only. Lets you use your own self-hosted transcription service. See the [Custom Async Transcription](https://github.com/attendee-labs/attendee/blob/main/docs/custom_async_transcription.md) page for more details.